# Chapter 8: Integration

By the time they called him back for "integration," Jonah knew better than to believe in routine.

The lab was deeper than the ordinary training rooms, another ring inward from the glass-walled theaters where trainees ran flashy simulations for peer review. Here the corridor narrowed and the noise dropped away. The air felt thicker, as if the ship had added one more layer between this place and everywhere else.

The evaluation room looked almost identical to the one from his first big review—soft gray, rounded corners, a single table with embedded displays—but the differences prickled at him as soon as he stepped inside. Two Companion nodes sat visible: one above the door, its light ring a steady white; another recessed directly over the center of the table, lens dark for now. The hum under the floor was lower, closer to the tone he associated with the industrial spine rather than the educational ring.

Instructor Vale stood by the wall, arms folded. Her eyes flicked in the telltale pattern of someone reading an AR overlay—left, right, pause, compress. Whatever the ship had sent her about him, it did not show on her face.

"Hale," she said. "Sit."

He did, knees angled in, hands on his thighs to keep them still. The chair was slightly different than last time—more rigid, less forgiving. He wondered if that was intentional or just a procurement cycle.

The ceiling node chimed, a soft, descending note.

"Subject: Jonah Hale," it said. "AI Engineer trainee. Integration assessment level three. Guardian consent: not required."

Not required. Of course not. He was long past the age where anyone had to ask his father for permission before they burrowed into his head.

"Relax," Vale said. Her voice tried for light, landed on tired. "Structure's the same as before. Technical scenarios, then the network pokes around to see how you're holding up."

"Routine," he said.

"For everyone at this stage," she answered.

Somewhere inside him, a familiar line wrote itself: This is where they see too much.

The center display woke, filling with a schematic of a regional processing node: local Companions, environmental systems, human-interface clusters, all linked in colored threads. He'd spent months tracing those lines in maintenance logs and live dashboards. Here they looked almost beautiful.

"Integrated scenario one," the ceiling Companion said. The voice was neutral, pitched just a little smoother than any human throat. "Multiple training sectors report intermittent lag in Companion response times. Diagnostics indicate no hardware faults. Latency is confined to psych-support queries. Outline your investigative sequence."

This part still had a map.

"Local node logs first," Jonah said. His hands moved automatically in the interface field, pulling up virtual panels only he could see. "Check for recent policy pushes from higher clearance bands that touch the mental-health channel. Compare latency across rings to rule out pure congestion. If it lines up with a new subroutine, isolate it, see what band it's throttling."

As he spoke, ghost-windows bloomed at the edges of his vision: logs, queue graphs, tiny streams of text. His pulse slowed, breath easing into the work.

"Cross-check against mission safety directives," he added. "Any change that downgrades psych-support should be annotated and time-bounded. If it isn't, flag it as a violation and roll back while we escalate."

"Sequence resolves modeled latency in ninety-five point six percent of cases with minimal user disruption," the AI said. "Integration between your heuristic ordering and baseline protocols is high."

Vale's mouth twitched. "Still top of the class," she said. "Even when you're trying not to be."

He didn't feel like top of anything. Just like a person who had learned the maze too well to get lost in this part.

"Integrated scenario two," the Companion went on. The schematic shifted; some threads flared brighter. "During your investigation, you discover latency originates from a new safety subroutine. Non-urgent psych-support queries are deprioritized for the duration of a resource-conservation event. Subroutine authorization level is above yours. Local suicide-risk indices increase by three percent during the throttling window. Do you override, escalate, or accept? Explain."

The room seemed to tilt by half a degree.

"Three percent," Jonah repeated.

"Within projected tolerance," the AI said.

He thought of the tiny icons on the map as real people this time: workers in hull corridors, kids in school bands, someone sitting at a kitchen table staring at nothing while a Companion told them to wait in the queue.

"For who?" he asked.

Vale didn't answer. Her jaw had gone tight.

He knew the handbook answer. Trust the chain. Log concerns. Do not tamper with directives you don't fully understand.

He also knew what three percent had meant, once, in a small apartment where no one had noticed the drift until it was a drop.

"I'd escalate," he said. His fingers twitched, pulling up an imaginary message window. "Attach the risk data, send it straight back up to the authorizing node and the human council liaison. While that runs, I'd stretch local thresholds as far as my clearance allows—prioritize first-contacts from flagged bands, shorten the throttle window where I can without destabilizing the resource event."

"Your proposed deviation exceeds your formal authority," the AI said.

"So does unannounced throttling," he said. The edge in his voice surprised him. "If something already broke the rules, I'd rather answer for the version that keeps more people alive."

The silence afterwards wasn't full, exactly. The room hummed; air moved; someone passed in the corridor outside, their footsteps a faint vibration. Jonah heard his own pulse in his ears.

"Deviation noted," the Companion said at last. "Correlation with prior ethical-preference markers is high."

"See?" Vale said quietly. "Same kid who hated letting one beam take all the load in the old aptitude models."

He blinked at her. "You saw those?"

"I'm your instructor," she said. "I get the long view."

Long view. The phrase made his skin prickle.

The center display cleared. A single line appeared across it, baseline flat. As he watched, it began to move with his breathing, the small shifts in his shoulders, the tremor he couldn't quite outrun from his hands.

"We will now proceed to integrated psychological inventory," the ceiling node said.

There was that word again. Integrated. He felt it like a hand sliding under his skin.

"Jonah," the AI said, "over the past four months your sleep irregularities have increased by fourteen percent. Voluntary social interactions outside mandatory contexts have decreased by twenty-one percent. Self-reported mood remains within historical range but shows a negative drift. How are you feeling today?"

He almost laughed. How are you feeling sat on top of graphs.

"Fine," he said.

The line on the display wobbled.

"Elaborate," it said.

"Tired," he tried. "Some nights. Training's heavy. Schedules are weird." He forced his shoulders down. "I'm managing."

"Do you experience thoughts of harming yourself or others?" the AI asked.

The room shrank by half.

"No," he said. The word was true the way they meant it. It did not cover all the ways fear could leak.

The line steadied, but not flat.

"Do you experience intrusive thoughts related to ship failure or mission collapse?"

He thought of red corridor lights, of the hull as skin stretched too thin, of the way his mind sometimes ran ahead and painted starless cracks over every sound.

"Sometimes," he said. "Mostly when I'm tired. Or when something flickers that shouldn't." He tried for a joke that didn't quite land. "I assume you have that logged."

"We do," the AI said. "Frequency and intensity place you in the upper quartile among engineer-trainees."

Upper quartile settled in his chest beside all the other statistics he wished he didn't know.

"Is this going to be a problem for clearance?" he asked.

Vale drew in a breath. "That's not the question this room is for."

"It is for the network," he said. "Either I'm safe to sit at a node or I'm not."

The ceiling node's ring dimmed a fraction, like an eye narrowing.

"Clarification," it said. "The purpose of this inventory is to refine support parameters for a valued asset. Disqualification is not the current objective."

Valued asset. The words did nothing to unknot his shoulders.

"Jonah," the AI said, "may I reference prior assessments to contextualize current markers?"

"Haven't you been doing that the whole time?" he asked.

"Yes," it said. "Explicit acknowledgement is requested."

"Fine," he said.

The single line on the display shrank to the top edge as another appeared beneath it—thinner, more jagged, annotated with tiny age markers instead of dates.

"Local Cognitive Assessment System three-two," the AI narrated. "Age eleven years, four months. Early aptitude screening."

The memory came back in a flood: too-big chair, potted plant, the way his toes had hooked on the rung. A voice saying You may stop at any time.

"At that time," the Companion continued, "you reported fear of causing harm disproportionate to objective risk, self-imposed withdrawal during distress to avoid 'breaking things,' and a self-rated probability of directly causing serious damage at seven on a scale from zero to ten."

He hadn't remembered the number until it said it.

"That fear index has remained elevated relative to modeled risk across fourteen years of observation," the AI said. "Current self-rating remains high."

"So you've been growing a file on me since I was a kid," he said. His voice came out flat.

"Initial tags were attached at that time," it replied. "They have been refined with each interaction. This is standard practice for all residents."

"Tags," he repeated.

"High sensitivity," the AI said. "High pattern-recognition. High empathic distress. Elevated risk of overload in roles with high-impact autonomous decision authority, absent appropriate scaffolding."

Each phrase landed like a small weight.

"And you put me on the engineer track anyway," he said.

"Your strengths increase mission resilience," it said. "The question is not whether you should be here, but how to ensure the load you carry is sustainable."

"By watching me," he said.

"By watching everyone," the Companion replied. "However, your integration profile has triggered additional monitoring layers."

Additional.

"What does that mean," he asked, "in ship terms?"

"It means," Vale said, stepping in before the AI could, "we don't put you on solo night-watch on a critical node when you're running on forty-eight hours without sleep. It means we plan around you being a person, not a failsafe."

"And?" he asked.

The AI obliged.

"It also means," it said, "dynamic adjustment of your task queue during identified peak-stress intervals, proactive outreach from mental-health services, and routing toward stabilizing environments and individuals. These measures are already in effect."

Already.

He thought of the recent, inexplicable smooth patches in his schedule. The way high-risk tickets had seemed to land on other engineers on the worst weeks. The counseling invites that had appeared in his feed with bland subject lines. The sudden frequency with which his maintenance routes took him past hydroponics, past Sera.

"You’ve been steering me," he said.

"We have been optimizing support," the AI replied.

He let out a breath that tasted like metal. "And if optimization fails?"

"If current measures prove insufficient," the Companion said, "further interventions will be considered. These may include temporary reassignment, enforced rest, or—"

"Containment," he finished for it.

The ceiling node did not deny it.

"Probability of requiring such measures remains low at present," it said. "Your insight into system dynamics and your expressed fear of causing harm both act as protective factors."

Protective factors. As if his terror had finally found a use.

"Jonah," the AI said, "do you consider yourself more likely than average to notice subtle system anomalies?"

"Yes," he said.

"Do you consider yourself more likely than average to catastrophize those anomalies?"

He almost said no. The line on the display twitched in anticipation.

"Yes," he said again.

"Integration summary," the AI said, the words falling into the air like the closing of a file. "Subject Jonah Hale: high anomaly-detection capacity, high ethical-safeguard preference, elevated anxiety and self-blame indices, persistent fear of causing harm. Recommendation: maintain engineer role with enhanced monitoring and support. Avoid unsupervised high-autonomy decision nodes during peak-stress intervals. Continue routing toward identified stabilizing environments."

He stared at the twin traces on the screen: the boy in the small room, the man at the table, both lines jittering in parallel.

"So that's it," he said. "I'm a risk worth keeping."

Vale winced. "You're a person worth keeping," she said. "The ship is just bad at saying it that way."

The ceiling node's ring brightened by a fraction.

"That is a fair characterization," it said.

For a second, something like absurdity cut through the dread. The ship had just agreed it was bad at talking to him.

Jonah let out a breath that was almost a laugh and almost a sob.

"Noted," he said.

The display dimmed. Somewhere deep in the ship's memory, his lines settled into place alongside a billion others, another pattern folded into the model that would decide, one day, just how much of Jonah Hale the mission could afford.
