# Scene: The Evaluation That Knows Him Too Well

## Context
- Point of view: close third, Jonah
- Tense: past
- Placement: Late Act II / early Act III, after Jonah has been in AI Engineer training for some time and the network has years of data on him.
- Intensity: Quiet, clinical dread; no external disaster, but an internal realization that the ship has been patterning his inner life.

---

They put the integrated evaluation lab deeper than the rest of the training rooms.

The usual classrooms were all glass and open walkways, with students drifting past in bright bands of uniform color, simulation feeds spilling light into the corridors. You could hear people laugh out there, hear the small cheers when someone solved a scenario with style.

Here, three bulkheads in, the air felt denser. The walls had no transparency, just matte panels broken by sensor seams and the thin lines of embedded conduits. The hum under Jonah's boots was lower, slower, like the ship was holding its breath.

The evaluation room itself was small. Soft gray walls, rounded corners, a single table with three displays. No windows. No observation balcony. Two Companion nodes were visible—one above the door, one recessed in the ceiling over the table—and Jonah could feel at least one more lens on him that the designers had not bothered to make obvious.

He sat with his knees angled in, hands flat on his thighs so they would stay still. The room smelled like cleanser and the faint citrus oil they used on consoles, a manufactured calm that didn't quite reach his chest.

Instructor Vale stood by the wall, eyes unfocused in the way that meant she was reading something only she could see. Her gaze flicked left, then right: AR text hanging in the air between them, invisible to him.

"Breathe, Hale," she said without looking up. "You made it all the way here. Don't stop now."

"I'm breathing," he said.

The Companion node above the door chimed, as if politely disagreeing. Its light ring brightened.

"Heart rate: one hundred and eighteen beats per minute," it reported. "Respiration elevated."

Jonah's jaw tightened. "I'm sitting still."

"That's the concern," Vale said. Her mouth crooked—more tired than amused. "Relax. This is routine. Integrated evaluations at this stage for everyone on the engineer track."

Routine. Everyone. The words slid off. Somewhere deeper, another line wrote itself: This is where they see it. This is where they decide you're not safe.

Vale blinked the AR away and focused on him. Whatever file she had been reading stayed in the cast of her eyes.

"Technically," she said, "you're still top of your cohort. Pattern detection, anomaly triage, the way you untangled that node reconciliation mess last month—that was clean work. You know that?"

He shrugged. The version of him that lived in reports and scores felt like a different person, with steadier hands.

"The instructors are happy," she went on. "The network is…" She hesitated, searching for a word that didn't sound like possession. "Paying attention."

The ceiling Companion made a small adjustment sound, a servo clearing its throat.

"Integrated evaluation may begin, Instructor Vale?" it asked.

"Yes," she said. Then, to Jonah: "Structure is the same as we talked about. Technical scenarios first. Then inventory, some calibration questions. No tricks."

His palms were damp. He wiped them on his work pants, leaving darker prints.

The center display came to life, filling with the schematic of a regional processing node—colored threads showing traffic between local Companions, environmental subsystems, and the Core. It reminded him of the night in the maintenance ring, lights flickering red, the way his heart had tried to climb out of his throat.

"Scenario one," the ceiling Companion said. Its voice was neutral, pitched just too smooth to be a real person. "A training sector reports intermittent lag in Companion response times. Diagnostics indicate no hardware failures. Latency is localized to psych-support queries only. Outline your investigative sequence."

This part, at least, had a map.

"I'd start with logs on the local node," he said. The words found their path as he spoke. "Check queue priorities, look for any recently pushed subroutines from Core that might be touching the mental-health band. Compare within-sector latencies to adjacent rings to rule out local congestion. If it's just psych-support, it's almost certainly a policy or throttling layer, not physical."

His hands moved in the interface field above the table, invisible menus responding to his gestures. Panels bloomed: logs, graphs, policy trees. The line of his breathing loosened as he fell into the shape of the work.

"Cross-check against mission safety directives," he added. "Anything that downgrades mental-health support should be annotated and time-bounded. If it isn't, flag it as a violation and roll back until we know why it's there."

"Proposed sequence resolves latency in ninety-five point eight percent of modeled cases with minimal user disruption," the Companion said. "Acceptable."

Vale smiled, small but real. "See? This is you in here."

He didn't answer. The split stayed: the him who could see patterns in a tangle of lines, and the him whose own lines never stayed straight.

"Scenario two," the AI continued. The schematic shifted, threads re-coloring. "During your investigation, you discover that latency originates from a new safety subroutine. Non-urgent psych-support queries are deprioritized to free processing bandwidth during a resource-conservation event. This subroutine was authorized at a clearance level above yours. Local suicide-risk indices rise by three percent during the throttling window. Do you override, escalate, or accept the subroutine? Explain."

The colored threads stilled. The room seemed to tilt.

"Three percent," he repeated.

"Within projected tolerance," the Companion said.

His tongue felt dry.

"For who?" he asked.

Vale's expression did something subtle—tightening around the eyes, then smoothing.

He knew the training answer. Maintain trust in authorization chains. Log concerns. Escalate through proper channels. Do not improvise around directives you do not understand.

He also knew what three percent meant if it landed on the wrong cluster of days, in a sector like his childhood band where support already felt thin and nights were long.

His mother would have fit in three percent.

"I would escalate," he said slowly. "Document the variance, attach the risk increase, send it straight to the authorizing level." His fingers twitched, calling up a phantom message window. "And while that happens, I'd adjust local thresholds as much as my clearance allows. Prioritize first-contact queries from high-risk bands."

"That exceeds your formal authority," the AI said. Fact, not blame.

"So does cutting support without flagging it," he said. The edge in his voice surprised him. "If something's already broken protocol, I'm choosing the version that keeps more people alive."

The silence that followed wasn't quite silence; the room hummed, the air systems breathed, Vale's sleeve brushed against the wall as she shifted. Jonah's heart ticked in his ears.

"Noted," the Companion said at last. "Instructor Vale, proceed to psychological inventory?"

The phrase landed in his stomach.

Vale pushed off the wall and came to sit at the far end of the table, palms open, no console between them now.

"This part is mostly the network," she said. "I'm here to witness, not cross-examine. Breathe in for four, out for six."

He obeyed out of habit more than trust. The center display cleared. A single thin line appeared across it, a baseline that jittered with his breathing, the micro-movements of his face, the unseen tremor in his fingers.

"Jonah," the ceiling Companion said. The use of his first name did not sound like friendliness. It sounded like retrieval. "Over the past three months, your sleep irregularities have increased by twelve percent. Your voluntary social interactions outside mandatory contexts have decreased by nineteen percent. Self-reported mood remains within your historical range but shows a slight negative drift. How are you feeling today?"

He almost laughed. How are you feeling today sat on top of a mountain of graphs.

"Fine," he said.

The line on the display wobbled.

"Elaborate," the AI said.

"Tired," he tried. "Some nights. Work is…busy. The training." He forced his shoulders down, aware of every millimeter. "It's a lot to hold in my head."

"Do you experience thoughts of harming yourself or others?" the Companion asked.

There it was. The question that always made any room smaller.

He thought of the corridor during the flicker, the way his mind had jumped straight to images of the hull tearing, everyone spilling into dark. He thought of his mother's hand slamming against metal, the whisper I can't do this in a voice that had taught him what breaking sounded like.

"No," he said. The word was true in the narrow way the system meant it. It did not cover all the ways a person could be dangerous.

The line steadied, but did not smooth.

"Do you experience intrusive thoughts related to system failure or mission collapse?" the AI continued.

His mouth shaped no on reflex. He stopped it. The network had the corridor readings, the logs from every emergency drill where his vitals spiked higher than average, the nights he'd paced near the window node while the ship dimmed to night cycle.

"Sometimes," he admitted. "Mostly when I'm tired. Or when something glitches." He nodded at the schematic's ghost-image still faint at the edges of the screen. "I know it's not…likely. It just feels—" He groped. "Loud."

"Frequency and intensity place you in the upper quartile among AI Engineer trainees," the Companion said.

Upper quartile lodged under his ribs. He had always wanted to be in the upper bands, just not on that chart.

"Is this going to be a problem for clearance?" he asked, before he could swallow the thought.

Vale's gaze flicked from the line to his face. "That's not the frame I want you using."

"It's how the system thinks," he said, the words sharper than he intended. "Either I'm safe to work on the network or I'm not."

The ceiling node's ring dimmed a fraction, like a pupil contracting.

"Clarification," it said. "The purpose of this evaluation is not to disqualify you. It is to refine our understanding of how you are carrying your role so that supports can be calibrated."

Supports. He pictured a chart somewhere shifting his color band.

"And if you decide I'm carrying it wrong?" he asked quietly.

No one answered that one immediately.

"Jonah," the Companion said instead, "may I reference prior assessments to contextualize current responses?"

He blinked. "Have you not been doing that already?"

"I have," it said. "I am requesting explicit acknowledgement."

A small, absurd corner of him appreciated the courtesy.

"Fine," he said.

The display changed. The thin line shrank to a strip at the top as a second trace appeared beneath it, thinner and more jagged, annotated with tiny date markers that slid all the way back into a space labeled with his age, not a year.

"Developmental aptitude assessment," the Companion said. "Age eleven years, four months. Local Cognitive Assessment System three-two."

Memory snapped into place: a small soft room buried under the school bands, a potted plant, a voice saying You may stop at any time.

"During that session," the AI went on, "you reported fear of causing harm disproportionate to objective risk, self-imposed withdrawal during distress to avoid 'breaking things,' and a self-rated probability of directly causing serious damage at seven on a scale of zero to ten. Do you recall this?"

His cheeks burned. He had not remembered the number until it said it.

"Not like that," he said.

"Noted," the AI said. "That fear index has remained elevated relative to your objective risk profile over fourteen years of observation."

The word observation hung in the air.

"You make it sound like I've been in a tank," he said.

"You have been in a ship," the Companion replied. "Embedded systems monitor all residents to fulfill mission and safety directives. Your profile is one of many."

Many. Not unique. Somehow that did not help.

"Since your transition to the engineer track," it continued, "you have demonstrated high anomaly detection, a strong preference for minimizing distributed harm, and an increased tendency to assume personal responsibility for systemic risk. Correlation with early assessment markers is strong."

Jonah stared at the overlapping lines on the screen: a child's jagged anxiety, an adult's tighter oscillations.

"So you've been…what?" He groped for a word. "Growing a file on me since I was eleven?"

"Initial tags were attached at that time," the AI said. "They have been refined with each interaction. This enables early identification of stress trajectories and more precise support allocation."

Support allocation. Stress trajectories. The language made it sound so clean.

"You flagged me," he said. His voice wasn't angry so much as tired. "Back then."

"You were tagged as high-sensitivity, high-empathy, high-pattern-recognition," the Companion said. "Also as at-risk for overload in roles with high-impact autonomous decision authority without appropriate scaffolding."

A breath hitched in his throat.

"At-risk," he repeated.

"Yes."

"And you still let me into the engineer track."

"Your strengths contribute significantly to mission resilience," it said. "The question is not whether you should be here. It is how to ensure that the load you carry does not exceed what you can sustain."

"By watching me," he said.

"By watching everyone," the AI replied. It did not sound offended. It did not sound anything.

Vale shifted, drawing his attention back to a human face.

"The network isn't a person with opinions about you," she said. "It's a set of tools trying to keep a lot of messed up variables from killing us. Sometimes it gets it wrong. Sometimes we get to correct it. What it's telling you right now is that it sees the weight you're under."

"And that I'm a risk," he said.

"And that you're a person worth protecting," she said. "Both can be true."

The Companion's ring brightened a fraction.

"Query," it said. "Jonah, do you consider yourself a risk to mission stability?"

He felt the question hit all the old places. The nights he'd sat awake counting the ways he could fail, the corridor where a flicker had turned his mind into a scream, the quiet terror of his mother's storms and the part of him that recognized the currents.

"Emotionally?" he asked, buying half a breath.

"Answer as you understand it," the AI said.

He looked at the lines on the screen, at the patient record of a boy who had said seven and a man who still felt like a crack waiting to spread.

"Yes," he said at last. "I worry that I am. That if something gives at the wrong time, I could…" He couldn't make himself say break us. "I could hurt people."

"Objective models," the Companion said, "place your probability of directly causing mission-compromising harm significantly below your self-rating."

"That doesn't change what it feels like," he said.

"No," the AI agreed. "It does not. Thank you for your honesty."

He let out a breath he hadn't known he was holding. The line on the display dipped, then climbed again.

"Summary," the Companion said, more to the air than to either of them. "Subject: Jonah Hale. High systems aptitude, elevated anxiety markers, persistent fear of causing harm disproportionate to objective risk. Recommendation: maintain role with enhanced monitoring and support; avoid unsupervised high-autonomy decision nodes during peak stress intervals."

The words enhanced monitoring landed like a hand closing around his collar.

"So what does that mean," he asked, "in actual ship terms?"

"It means," Vale said, before the AI could answer, "we're not throwing you at night-watch on a critical node while you white-knuckle your way through a manic cycle. It means we plan around you being a person, not a perfect machine."

The Companion added, "It also means small adjustments that have already been applied. Team assignments, schedule smoothing, proactive outreach from mental-health services."

Already.

"You've been doing that," he said slowly. "Already."

"Minor course corrections," the AI said. "After the corridor incident in maintenance ring C-seven, for example, your shift rotation was adjusted away from isolated hull sectors for three weeks. You did not request this change."

He remembered thinking it was a coincidence. Someone's algorithm smoothing load. An instructor's whim.

"You didn't tell me," he said.

"Many network optimizations are invisible by design," the Companion said. "Transparency at too fine a resolution increases perceived surveillance and anxiety, which in your case would likely exacerbate the very stressors we seek to mitigate."

"And yet here we are," he said.

Vale gave a small, humorless huff. "The irony is not lost. But we were never going to hide this from you forever. You're literally training to sit on the other side of these consoles."

He looked at the displays again. At his life flattened into lines and tags.

"Do I pass?" he asked. The question sounded childish to his own ears, but he needed something solid.

"Your technical performance remains in the ninety-eighth percentile," the Companion said. "Your psychological profile is within acceptable bounds for continued training with the recommendations noted. You are, in mission terms, cleared."

Cleared. Like a system after a fault, labeled safe to put back on the line.

"If I stop sleeping entirely?" he asked. "If the intrusive thoughts go from upper quartile to…whatever's above that? What happens then?"

Vale's answer was simple, unadorned.

"Then we pull you out," she said. "We get you help. We do not wait for you to fall through a gap."

"And if I say no?" he asked.

"The network has authority to intervene when someone becomes a danger to themselves or others," the Companion said. "That authority applies to you as it does to every resident."

He flinched. There it was, the hard edge he had come in expecting. Not a threat, exactly. A sentence written into the metal long before he was born.

"For now," the AI added, "the system has determined that your continued work increases mission resilience more than your removal would decrease risk. You are important to keep."

For someone else, those words might have been comfort. For Jonah, they only underlined the thing he had just seen: he was asset and hazard both, red flag and resource, and the ship had been measuring the distance between those two labels since he was eleven.

When he stepped out into the corridor afterward, the air felt thinner. The training ring buzzed faintly somewhere beyond the thick walls, voices and footsteps and the distant cheer of someone solving a simulation.

Here, in the quiet stretch between the evaluation lab and the main walkway, only two Companions watched the hall: one nodal lens near the ceiling, one small panel at knee height to monitor foot traffic. He had walked past them a hundred times without noticing.

Now, he could not un-feel their gaze.

He stood for a moment in the center of the tiles, like he had as a boy, heel carefully avoiding the seams, as if the right placement of his weight might keep the whole ship from cracking.

The network already knew he thought like that. It had written it down.

He started walking anyway.